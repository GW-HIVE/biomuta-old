1) get list of samples from gdc 
gdc advanced search

#Primary Tumor
"""
files.analysis.workflow_type in ["HTSeq - Counts"]  and files.data_type in ["Gene Expression Quantification"] and cases.samples.sample_type in ["Primary Tumor"] and cases.project.program.name in ["TCGA"]
"""
9760 files added all to cart and downloaded - if this number exceeds 10k will need to find new method because cart size is capped

downloaded files:
    a) gdc_sample_sheet.primary_tumor.tsv (sample sheet from cart)
    b) metadata.cart.primary_tumor.json (metadata)


#Solid Tissue Normal
"""
files.analysis.workflow_type in ["HTSeq - Counts"]  and files.data_type in ["Gene Expression Quantification"] and cases.samples.sample_type in ["Solid Tissue Normal"] and cases.project.program.name in ["TCGA"]
"""
730 files added all to cart and downloaded - if this number exceeds 10k will need to find new method because cart size is capped

downloaded files:
    a) gdc_sample_sheet.solid_tissue_normal.tsv (sample sheet from cart)
    b) metadata.cart.solid_tissue_normal.json (metadata)
    c) gdc_manifest_solid_tissue_normal.txt (manifest)


#note had to change metadata, manifest, and sample sheet file names, name included date of download but not tissue type
These files were downloaded on March 6, 2019



2) download all samples from gdc
wrapper script 
get_data_all_samples.sh
runs get_data_all_samples.py
downloads all tarballs in ../files_tcga/${proj_id}/${sam_type}
logs of number of downloaded files and number of duplicate files (none) saved in get_data_all_samples.log




3) unzip the downloads copy individual files into intermediate directory
get_hits_into_dir.py
get_hits_into_dir.log
all direcotory information is hard coded...
files are copied into /home/bfochtman/bioXpress/downloads/files_tcga/${tcga_study}_${sample_type}_intermediate/
only creates intermediate files for those tcga studies with greater than or equal to 10 files of each sample type




4) compile count matrix and category table 

wrapper file
merge_files_tumor_and_normal.sh
calls
merge_files_tumor_and_normal.py
the python script takes a string describing the intermediate files as input and merges all files contained therein
this script also filters out row names starting with "_"
the list in the wrapper file was compiled manually from those studies for which intermediate files were generated in step 3

5) Download from glygen datasets and place them under downloads/uniprotkb

    a) wget https://data.dev.glygen.org/ln2wwwdata/reviewed/human_protein_idmapping.csv  --no-check-certificate
    b) wget https://data.dev.glygen.org/ln2wwwdata/reviewed/human_protein_xref_refseq.csv  --no-check-certificate
    c) wget https://data.dev.glygen.org/ln2wwwdata/reviewed/human_protein_site_annotation.csv  --no-check-certificate

